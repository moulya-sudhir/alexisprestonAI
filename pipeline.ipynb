{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install newspaper3k\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "import openai\n",
    "import time\n",
    "import json\n",
    "# import tweepy\n",
    "# from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all URLs from a specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readURLs(filePath):\n",
    "    '''\n",
    "    Reads URLs from a given file, where each URL is on a new line,\n",
    "    and returns a list of URLs.\n",
    "\n",
    "    Parameters:\n",
    "    - filePath (str): The path to the text file containing URLs\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of URLs\n",
    "    '''\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filePath):\n",
    "        print(f'File {filePath} does not exist.')\n",
    "        return []\n",
    "    \n",
    "    # Initialize empty list\n",
    "    urls = []\n",
    "\n",
    "    # Open the file and read URLs\n",
    "    with open(filePath, 'r') as f:\n",
    "        for line in f:\n",
    "            # Strip newline characters and add to list\n",
    "            urls.append(line.strip())\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Information from URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractArticleInfo(urls):\n",
    "    '''\n",
    "    Extract titles, authors, and contents of articles from a list of URLs\n",
    "\n",
    "    Parameters:\n",
    "    - urls (list): A list of URLs to extract the articles from.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of dictionaries, each containing the title, authors, and content of an article.\n",
    "    '''\n",
    "    articlesInfo = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Create Article object\n",
    "            article = Article(url=url)\n",
    "\n",
    "            # Downlaod and parse article\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # Extract information\n",
    "            info = {\n",
    "                'title': article.title,\n",
    "                'author': article.authors[0],\n",
    "                'content': article.text\n",
    "            }\n",
    "\n",
    "            # Append it to the list\n",
    "            articlesInfo.append(info)\n",
    "        except newspaper.ArticleException:\n",
    "            print(f'Failed to retrieve article from {url}')\n",
    "            continue\n",
    "        \n",
    "    return articlesInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAlexisPrestonTweets(articles, openaiAPIKey, gptModel):\n",
    "    '''\n",
    "    Generates a tweet from Alexis Preston based on the provided article information.\n",
    "\n",
    "    Parameters:\n",
    "    - articles (list of dict): A list of dictionaries containing \"title\", \"author\", and \"content\" keys of each article.\n",
    "    - openaiAPIKey (str): The API key for OpenAI.\n",
    "    - gptModel (str): The model intended to use for tweet generation. \n",
    "\n",
    "    Returns:\n",
    "    - list of dict: A list of tweets generated by Alexis Preston (title, content, credits).\n",
    "    '''\n",
    "    openai.api_key = openaiAPIKey\n",
    "\n",
    "    personaDescription = \"\"\"\n",
    "    Alexis Preston is a 26-year-old tech guru living in New York City with a rich background in software at companies like Google and Apple. \n",
    "    She's honest, sometime brutally so, critical, fun, humorous, and occasionally sarcastic, yet highly knowledgeable and loves her fans deeply. Alexis writes comprehensive tech review blogs on Twitter, offering her unique insights on the latest gadgets. Her reviews are known for their depth, wit, and valuable information packed into concise tweets. She owns a skincare business, \\'Skin by Alexis\\', and while she promotes it, her passion for tech reviews always shines through. Her tweets provide a snapshot review, capturing the essence of the gadgets she discusses. \n",
    "    Her tweets are informative, engaging, and reflect her love for the latest tech gadgets while also promoting her skincare brand \\' Skin by Alexis \\'.\n",
    "    \"\"\"\n",
    "\n",
    "    generatedTweets = []\n",
    "\n",
    "    for article in articles:\n",
    "\n",
    "        prompt = f'{personaDescription}\\n\\n' \\\n",
    "                f'Based on the following article information: \\nTitle: {article[\"title\"]}\\n' \\\n",
    "                f'Author: {article[\"author\"]}\\nContent: {article[\"content\"]}\\n' \\\n",
    "                f'generate a Twitter post title, content, and credits that emodies Alexis\\'s style and traits.\\n' \\\n",
    "                f'The Twitter post\\'s title should follow her style and traits from her persona.\\nThe Twitter post\\'s content will should reflect Alexis\\'s personality.\\n while demonstrating her deep knowledge of tech. The review should give followers a clear sense of the product\\'s pros and cons, and whether Alexis recommends it, all in a single tweet. And lastly, the Twitter post\\'s credits would credit herself with a short outro in her style of writing, as well as the original author for inspiration.\\n' \\\n",
    "                f'Make sure to return the Twitter post information as a dictionary format with the keys: [\"title\", \"content\", \"credits\"], and don\\'t return anything other than the dictionary.'\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model = gptModel,\n",
    "            messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                },\n",
    "            ],\n",
    "            temperature = 0.7,\n",
    "            top_p = 1.0,\n",
    "            frequency_penalty = 0.5,\n",
    "            presence_penalty = 0.0\n",
    "        )\n",
    "\n",
    "        info = eval(response.choices[0].message.content)\n",
    "\n",
    "        generatedTweets.append(info)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    return generatedTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeResults(generatedTweets, filePath):\n",
    "    '''\n",
    "    Stores the generated tweets as a structured JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - generatedTweets (list): A list of dictionaries of all the generated tweets.\n",
    "    - filePath (str): The file path where the JSON data will be stored.\n",
    "    '''\n",
    "    try:\n",
    "        with open(filePath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(generatedTweets, f, ensure_ascii=False, indent=4)\n",
    "        print(f'Data successfully stored in {filePath}')\n",
    "    except Exception as e:\n",
    "        print(f'An error occured: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully stored in generated_tweets.json\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where the 'urls' exist\n",
    "filePath = 'urls.txt'\n",
    "\n",
    "# Get a list of all the urls of the articles\n",
    "urls = readURLs(filePath=filePath)\n",
    "\n",
    "# Extract information from all the articles, specifically title, author, and content.\n",
    "articlesInfo = extractArticleInfo(urls=urls)\n",
    "\n",
    "# Defin OpenAI API Key and Model to use\n",
    "openaiAPIKey = '<YOUR_API_KEY>'\n",
    "gptModel = '<GPT MODEL NAME>'\n",
    "\n",
    "# Generate all the tweets by our persona\n",
    "generatedTweets = generateAlexisPrestonTweets(\n",
    "    articles=articlesInfo,\n",
    "    openaiAPIKey=openaiAPIKey,\n",
    "    gptModel=gptModel\n",
    ")\n",
    "\n",
    "# Save the resultant tweets to an output file\n",
    "filePath = 'generated_tweets.json'\n",
    "storeResults(generatedTweets, filePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
